import json, strutils, streams
import ./matOps
let
    weightsStream = newFilestream("./stablelm_1_6b_model/stable_lm_1_6b_8bdf317e2b35ab5c8009cbb6c7ce495e4e608a6b9b843d44054edf25b8c5860d.safetensors", fmRead)

var
    OFFSET: int
    weightsInfo: JsonNode

if not(isNil(weightsStream)):
    var 
        headerLen: uint64

    weightsStream.peek(headerLen)#see the  headerLength

    OFFSET = int(headerLen + 8)

    var weightsInfoStr = newString(headerLen.int)

    weightsStream.setPosition(8)
    assert weightsStream.readDataStr(weightsInfoStr, 0..<headerLen.int) == headerLen.int, " couldnt read all the weightsInfo"

    weightsInfo = parseJson(weightsInfoStr)

type Weights = enum
    Lm_head_weight = "lm_head.weight"
    Embed_tokens_weight = "model.embed_tokens.weight"
    Layer_0_input_layernorm_bias = "model.layers.0.input_layernorm.bias"
    Layer_0_input_layernorm_weight = "model.layers.0.input_layernorm.weight"
    Layer_0_mlp_down_proj_weight = "model.layers.0.mlp.down_proj.weight"
    Layer_0_mlp_gate_proj_weight = "model.layers.0.mlp.gate_proj.weight"
    Layer_0_mlp_up_proj_weight = "model.layers.0.mlp.up_proj.weight"
    Layer_0_post_attention_layernorm_bias = "model.layers.0.post_attention_layernorm.bias"
    Layer_0_post_attention_layernorm_weight = "model.layers.0.post_attention_layernorm.weight"
    Layer_0_self_attn_k_proj_bias = "model.layers.0.self_attn.k_proj.bias"
    Layer_0_self_attn_k_proj_weight = "model.layers.0.self_attn.k_proj.weight"
    Layer_0_self_attn_o_proj_weight = "model.layers.0.self_attn.o_proj.weight"
    Layer_0_self_attn_q_proj_bias = "model.layers.0.self_attn.q_proj.bias"
    Layer_0_self_attn_q_proj_weight = "model.layers.0.self_attn.q_proj.weight"
    Layer_0_self_attn_v_proj_bias = "model.layers.0.self_attn.v_proj.bias"
    Layer_0_self_attn_v_proj_weight = "model.layers.0.self_attn.v_proj.weight"
    Layer_1_input_layernorm_bias = "model.layers.1.input_layernorm.bias"
    Layer_1_input_layernorm_weight = "model.layers.1.input_layernorm.weight"
    Layer_1_mlp_down_proj_weight = "model.layers.1.mlp.down_proj.weight"
    Layer_1_mlp_gate_proj_weight = "model.layers.1.mlp.gate_proj.weight"
    Layer_1_mlp_up_proj_weight = "model.layers.1.mlp.up_proj.weight"
    Layer_1_post_attention_layernorm_bias = "model.layers.1.post_attention_layernorm.bias"
    Layer_1_post_attention_layernorm_weight = "model.layers.1.post_attention_layernorm.weight"
    Layer_1_self_attn_k_proj_bias = "model.layers.1.self_attn.k_proj.bias"
    Layer_1_self_attn_k_proj_weight = "model.layers.1.self_attn.k_proj.weight"
    Layer_1_self_attn_o_proj_weight = "model.layers.1.self_attn.o_proj.weight"
    Layer_1_self_attn_q_proj_bias = "model.layers.1.self_attn.q_proj.bias"
    Layer_1_self_attn_q_proj_weight = "model.layers.1.self_attn.q_proj.weight"
    Layer_1_self_attn_v_proj_bias = "model.layers.1.self_attn.v_proj.bias"
    Layer_1_self_attn_v_proj_weight = "model.layers.1.self_attn.v_proj.weight"
    Layer_10_input_layernorm_bias = "model.layers.10.input_layernorm.bias"
    Layer_10_input_layernorm_weight = "model.layers.10.input_layernorm.weight"
    Layer_10_mlp_down_proj_weight = "model.layers.10.mlp.down_proj.weight"
    Layer_10_mlp_gate_proj_weight = "model.layers.10.mlp.gate_proj.weight"
    Layer_10_mlp_up_proj_weight = "model.layers.10.mlp.up_proj.weight"
    Layer_10_post_attention_layernorm_bias = "model.layers.10.post_attention_layernorm.bias"
    Layer_10_post_attention_layernorm_weight = "model.layers.10.post_attention_layernorm.weight"
    Layer_10_self_attn_k_proj_bias = "model.layers.10.self_attn.k_proj.bias"
    Layer_10_self_attn_k_proj_weight = "model.layers.10.self_attn.k_proj.weight"
    Layer_10_self_attn_o_proj_weight = "model.layers.10.self_attn.o_proj.weight"
    Layer_10_self_attn_q_proj_bias = "model.layers.10.self_attn.q_proj.bias"
    Layer_10_self_attn_q_proj_weight = "model.layers.10.self_attn.q_proj.weight"
    Layer_10_self_attn_v_proj_bias = "model.layers.10.self_attn.v_proj.bias"
    Layer_10_self_attn_v_proj_weight = "model.layers.10.self_attn.v_proj.weight"
    Layer_11_input_layernorm_bias = "model.layers.11.input_layernorm.bias"
    Layer_11_input_layernorm_weight = "model.layers.11.input_layernorm.weight"
    Layer_11_mlp_down_proj_weight = "model.layers.11.mlp.down_proj.weight"
    Layer_11_mlp_gate_proj_weight = "model.layers.11.mlp.gate_proj.weight"
    Layer_11_mlp_up_proj_weight = "model.layers.11.mlp.up_proj.weight"
    Layer_11_post_attention_layernorm_bias = "model.layers.11.post_attention_layernorm.bias"
    Layer_11_post_attention_layernorm_weight = "model.layers.11.post_attention_layernorm.weight"
    Layer_11_self_attn_k_proj_bias = "model.layers.11.self_attn.k_proj.bias"
    Layer_11_self_attn_k_proj_weight = "model.layers.11.self_attn.k_proj.weight"
    Layer_11_self_attn_o_proj_weight = "model.layers.11.self_attn.o_proj.weight"
    Layer_11_self_attn_q_proj_bias = "model.layers.11.self_attn.q_proj.bias"
    Layer_11_self_attn_q_proj_weight = "model.layers.11.self_attn.q_proj.weight"
    Layer_11_self_attn_v_proj_bias = "model.layers.11.self_attn.v_proj.bias"
    Layer_11_self_attn_v_proj_weight = "model.layers.11.self_attn.v_proj.weight"
    Layer_12_input_layernorm_bias = "model.layers.12.input_layernorm.bias"
    Layer_12_input_layernorm_weight = "model.layers.12.input_layernorm.weight"
    Layer_12_mlp_down_proj_weight = "model.layers.12.mlp.down_proj.weight"
    Layer_12_mlp_gate_proj_weight = "model.layers.12.mlp.gate_proj.weight"
    Layer_12_mlp_up_proj_weight = "model.layers.12.mlp.up_proj.weight"
    Layer_12_post_attention_layernorm_bias = "model.layers.12.post_attention_layernorm.bias"
    Layer_12_post_attention_layernorm_weight = "model.layers.12.post_attention_layernorm.weight"
    Layer_12_self_attn_k_proj_bias = "model.layers.12.self_attn.k_proj.bias"
    Layer_12_self_attn_k_proj_weight = "model.layers.12.self_attn.k_proj.weight"
    Layer_12_self_attn_o_proj_weight = "model.layers.12.self_attn.o_proj.weight"
    Layer_12_self_attn_q_proj_bias = "model.layers.12.self_attn.q_proj.bias"
    Layer_12_self_attn_q_proj_weight = "model.layers.12.self_attn.q_proj.weight"
    Layer_12_self_attn_v_proj_bias = "model.layers.12.self_attn.v_proj.bias"
    Layer_12_self_attn_v_proj_weight = "model.layers.12.self_attn.v_proj.weight"
    Layer_13_input_layernorm_bias = "model.layers.13.input_layernorm.bias"
    Layer_13_input_layernorm_weight = "model.layers.13.input_layernorm.weight"
    Layer_13_mlp_down_proj_weight = "model.layers.13.mlp.down_proj.weight"
    Layer_13_mlp_gate_proj_weight = "model.layers.13.mlp.gate_proj.weight"
    Layer_13_mlp_up_proj_weight = "model.layers.13.mlp.up_proj.weight"
    Layer_13_post_attention_layernorm_bias = "model.layers.13.post_attention_layernorm.bias"
    Layer_13_post_attention_layernorm_weight = "model.layers.13.post_attention_layernorm.weight"
    Layer_13_self_attn_k_proj_bias = "model.layers.13.self_attn.k_proj.bias"
    Layer_13_self_attn_k_proj_weight = "model.layers.13.self_attn.k_proj.weight"
    Layer_13_self_attn_o_proj_weight = "model.layers.13.self_attn.o_proj.weight"
    Layer_13_self_attn_q_proj_bias = "model.layers.13.self_attn.q_proj.bias"
    Layer_13_self_attn_q_proj_weight = "model.layers.13.self_attn.q_proj.weight"
    Layer_13_self_attn_v_proj_bias = "model.layers.13.self_attn.v_proj.bias"
    Layer_13_self_attn_v_proj_weight = "model.layers.13.self_attn.v_proj.weight"
    Layer_14_input_layernorm_bias = "model.layers.14.input_layernorm.bias"
    Layer_14_input_layernorm_weight = "model.layers.14.input_layernorm.weight"
    Layer_14_mlp_down_proj_weight = "model.layers.14.mlp.down_proj.weight"
    Layer_14_mlp_gate_proj_weight = "model.layers.14.mlp.gate_proj.weight"
    Layer_14_mlp_up_proj_weight = "model.layers.14.mlp.up_proj.weight"
    Layer_14_post_attention_layernorm_bias = "model.layers.14.post_attention_layernorm.bias"
    Layer_14_post_attention_layernorm_weight = "model.layers.14.post_attention_layernorm.weight"
    Layer_14_self_attn_k_proj_bias = "model.layers.14.self_attn.k_proj.bias"
    Layer_14_self_attn_k_proj_weight = "model.layers.14.self_attn.k_proj.weight"
    Layer_14_self_attn_o_proj_weight = "model.layers.14.self_attn.o_proj.weight"
    Layer_14_self_attn_q_proj_bias = "model.layers.14.self_attn.q_proj.bias"
    Layer_14_self_attn_q_proj_weight = "model.layers.14.self_attn.q_proj.weight"
    Layer_14_self_attn_v_proj_bias = "model.layers.14.self_attn.v_proj.bias"
    Layer_14_self_attn_v_proj_weight = "model.layers.14.self_attn.v_proj.weight"
    Layer_15_input_layernorm_bias = "model.layers.15.input_layernorm.bias"
    Layer_15_input_layernorm_weight = "model.layers.15.input_layernorm.weight"
    Layer_15_mlp_down_proj_weight = "model.layers.15.mlp.down_proj.weight"
    Layer_15_mlp_gate_proj_weight = "model.layers.15.mlp.gate_proj.weight"
    Layer_15_mlp_up_proj_weight = "model.layers.15.mlp.up_proj.weight"
    Layer_15_post_attention_layernorm_bias = "model.layers.15.post_attention_layernorm.bias"
    Layer_15_post_attention_layernorm_weight = "model.layers.15.post_attention_layernorm.weight"
    Layer_15_self_attn_k_proj_bias = "model.layers.15.self_attn.k_proj.bias"
    Layer_15_self_attn_k_proj_weight = "model.layers.15.self_attn.k_proj.weight"
    Layer_15_self_attn_o_proj_weight = "model.layers.15.self_attn.o_proj.weight"
    Layer_15_self_attn_q_proj_bias = "model.layers.15.self_attn.q_proj.bias"
    Layer_15_self_attn_q_proj_weight = "model.layers.15.self_attn.q_proj.weight"
    Layer_15_self_attn_v_proj_bias = "model.layers.15.self_attn.v_proj.bias"
    Layer_15_self_attn_v_proj_weight = "model.layers.15.self_attn.v_proj.weight"
    Layer_16_input_layernorm_bias = "model.layers.16.input_layernorm.bias"
    Layer_16_input_layernorm_weight = "model.layers.16.input_layernorm.weight"
    Layer_16_mlp_down_proj_weight = "model.layers.16.mlp.down_proj.weight"
    Layer_16_mlp_gate_proj_weight = "model.layers.16.mlp.gate_proj.weight"
    Layer_16_mlp_up_proj_weight = "model.layers.16.mlp.up_proj.weight"
    Layer_16_post_attention_layernorm_bias = "model.layers.16.post_attention_layernorm.bias"
    Layer_16_post_attention_layernorm_weight = "model.layers.16.post_attention_layernorm.weight"
    Layer_16_self_attn_k_proj_bias = "model.layers.16.self_attn.k_proj.bias"
    Layer_16_self_attn_k_proj_weight = "model.layers.16.self_attn.k_proj.weight"
    Layer_16_self_attn_o_proj_weight = "model.layers.16.self_attn.o_proj.weight"
    Layer_16_self_attn_q_proj_bias = "model.layers.16.self_attn.q_proj.bias"
    Layer_16_self_attn_q_proj_weight = "model.layers.16.self_attn.q_proj.weight"
    Layer_16_self_attn_v_proj_bias = "model.layers.16.self_attn.v_proj.bias"
    Layer_16_self_attn_v_proj_weight = "model.layers.16.self_attn.v_proj.weight"
    Layer_17_input_layernorm_bias = "model.layers.17.input_layernorm.bias"
    Layer_17_input_layernorm_weight = "model.layers.17.input_layernorm.weight"
    Layer_17_mlp_down_proj_weight = "model.layers.17.mlp.down_proj.weight"
    Layer_17_mlp_gate_proj_weight = "model.layers.17.mlp.gate_proj.weight"
    Layer_17_mlp_up_proj_weight = "model.layers.17.mlp.up_proj.weight"
    Layer_17_post_attention_layernorm_bias = "model.layers.17.post_attention_layernorm.bias"
    Layer_17_post_attention_layernorm_weight = "model.layers.17.post_attention_layernorm.weight"
    Layer_17_self_attn_k_proj_bias = "model.layers.17.self_attn.k_proj.bias"
    Layer_17_self_attn_k_proj_weight = "model.layers.17.self_attn.k_proj.weight"
    Layer_17_self_attn_o_proj_weight = "model.layers.17.self_attn.o_proj.weight"
    Layer_17_self_attn_q_proj_bias = "model.layers.17.self_attn.q_proj.bias"
    Layer_17_self_attn_q_proj_weight = "model.layers.17.self_attn.q_proj.weight"
    Layer_17_self_attn_v_proj_bias = "model.layers.17.self_attn.v_proj.bias"
    Layer_17_self_attn_v_proj_weight = "model.layers.17.self_attn.v_proj.weight"
    Layer_18_input_layernorm_bias = "model.layers.18.input_layernorm.bias"
    Layer_18_input_layernorm_weight = "model.layers.18.input_layernorm.weight"
    Layer_18_mlp_down_proj_weight = "model.layers.18.mlp.down_proj.weight"
    Layer_18_mlp_gate_proj_weight = "model.layers.18.mlp.gate_proj.weight"
    Layer_18_mlp_up_proj_weight = "model.layers.18.mlp.up_proj.weight"
    Layer_18_post_attention_layernorm_bias = "model.layers.18.post_attention_layernorm.bias"
    Layer_18_post_attention_layernorm_weight = "model.layers.18.post_attention_layernorm.weight"
    Layer_18_self_attn_k_proj_bias = "model.layers.18.self_attn.k_proj.bias"
    Layer_18_self_attn_k_proj_weight = "model.layers.18.self_attn.k_proj.weight"
    Layer_18_self_attn_o_proj_weight = "model.layers.18.self_attn.o_proj.weight"
    Layer_18_self_attn_q_proj_bias = "model.layers.18.self_attn.q_proj.bias"
    Layer_18_self_attn_q_proj_weight = "model.layers.18.self_attn.q_proj.weight"
    Layer_18_self_attn_v_proj_bias = "model.layers.18.self_attn.v_proj.bias"
    Layer_18_self_attn_v_proj_weight = "model.layers.18.self_attn.v_proj.weight"
    Layer_19_input_layernorm_bias = "model.layers.19.input_layernorm.bias"
    Layer_19_input_layernorm_weight = "model.layers.19.input_layernorm.weight"
    Layer_19_mlp_down_proj_weight = "model.layers.19.mlp.down_proj.weight"
    Layer_19_mlp_gate_proj_weight = "model.layers.19.mlp.gate_proj.weight"
    Layer_19_mlp_up_proj_weight = "model.layers.19.mlp.up_proj.weight"
    Layer_19_post_attention_layernorm_bias = "model.layers.19.post_attention_layernorm.bias"
    Layer_19_post_attention_layernorm_weight = "model.layers.19.post_attention_layernorm.weight"
    Layer_19_self_attn_k_proj_bias = "model.layers.19.self_attn.k_proj.bias"
    Layer_19_self_attn_k_proj_weight = "model.layers.19.self_attn.k_proj.weight"
    Layer_19_self_attn_o_proj_weight = "model.layers.19.self_attn.o_proj.weight"
    Layer_19_self_attn_q_proj_bias = "model.layers.19.self_attn.q_proj.bias"
    Layer_19_self_attn_q_proj_weight = "model.layers.19.self_attn.q_proj.weight"
    Layer_19_self_attn_v_proj_bias = "model.layers.19.self_attn.v_proj.bias"
    Layer_19_self_attn_v_proj_weight = "model.layers.19.self_attn.v_proj.weight"
    Layer_2_input_layernorm_bias = "model.layers.2.input_layernorm.bias"
    Layer_2_input_layernorm_weight = "model.layers.2.input_layernorm.weight"
    Layer_2_mlp_down_proj_weight = "model.layers.2.mlp.down_proj.weight"
    Layer_2_mlp_gate_proj_weight = "model.layers.2.mlp.gate_proj.weight"
    Layer_2_mlp_up_proj_weight = "model.layers.2.mlp.up_proj.weight"
    Layer_2_post_attention_layernorm_bias = "model.layers.2.post_attention_layernorm.bias"
    Layer_2_post_attention_layernorm_weight = "model.layers.2.post_attention_layernorm.weight"
    Layer_2_self_attn_k_proj_bias = "model.layers.2.self_attn.k_proj.bias"
    Layer_2_self_attn_k_proj_weight = "model.layers.2.self_attn.k_proj.weight"
    Layer_2_self_attn_o_proj_weight = "model.layers.2.self_attn.o_proj.weight"
    Layer_2_self_attn_q_proj_bias = "model.layers.2.self_attn.q_proj.bias"
    Layer_2_self_attn_q_proj_weight = "model.layers.2.self_attn.q_proj.weight"
    Layer_2_self_attn_v_proj_bias = "model.layers.2.self_attn.v_proj.bias"
    Layer_2_self_attn_v_proj_weight = "model.layers.2.self_attn.v_proj.weight"
    Layer_20_input_layernorm_bias = "model.layers.20.input_layernorm.bias"
    Layer_20_input_layernorm_weight = "model.layers.20.input_layernorm.weight"
    Layer_20_mlp_down_proj_weight = "model.layers.20.mlp.down_proj.weight"
    Layer_20_mlp_gate_proj_weight = "model.layers.20.mlp.gate_proj.weight"
    Layer_20_mlp_up_proj_weight = "model.layers.20.mlp.up_proj.weight"
    Layer_20_post_attention_layernorm_bias = "model.layers.20.post_attention_layernorm.bias"
    Layer_20_post_attention_layernorm_weight = "model.layers.20.post_attention_layernorm.weight"
    Layer_20_self_attn_k_proj_bias = "model.layers.20.self_attn.k_proj.bias"
    Layer_20_self_attn_k_proj_weight = "model.layers.20.self_attn.k_proj.weight"
    Layer_20_self_attn_o_proj_weight = "model.layers.20.self_attn.o_proj.weight"
    Layer_20_self_attn_q_proj_bias = "model.layers.20.self_attn.q_proj.bias"
    Layer_20_self_attn_q_proj_weight = "model.layers.20.self_attn.q_proj.weight"
    Layer_20_self_attn_v_proj_bias = "model.layers.20.self_attn.v_proj.bias"
    Layer_20_self_attn_v_proj_weight = "model.layers.20.self_attn.v_proj.weight"
    Layer_21_input_layernorm_bias = "model.layers.21.input_layernorm.bias"
    Layer_21_input_layernorm_weight = "model.layers.21.input_layernorm.weight"
    Layer_21_mlp_down_proj_weight = "model.layers.21.mlp.down_proj.weight"
    Layer_21_mlp_gate_proj_weight = "model.layers.21.mlp.gate_proj.weight"
    Layer_21_mlp_up_proj_weight = "model.layers.21.mlp.up_proj.weight"
    Layer_21_post_attention_layernorm_bias = "model.layers.21.post_attention_layernorm.bias"
    Layer_21_post_attention_layernorm_weight = "model.layers.21.post_attention_layernorm.weight"
    Layer_21_self_attn_k_proj_bias = "model.layers.21.self_attn.k_proj.bias"
    Layer_21_self_attn_k_proj_weight = "model.layers.21.self_attn.k_proj.weight"
    Layer_21_self_attn_o_proj_weight = "model.layers.21.self_attn.o_proj.weight"
    Layer_21_self_attn_q_proj_bias = "model.layers.21.self_attn.q_proj.bias"
    Layer_21_self_attn_q_proj_weight = "model.layers.21.self_attn.q_proj.weight"
    Layer_21_self_attn_v_proj_bias = "model.layers.21.self_attn.v_proj.bias"
    Layer_21_self_attn_v_proj_weight = "model.layers.21.self_attn.v_proj.weight"
    Layer_22_input_layernorm_bias = "model.layers.22.input_layernorm.bias"
    Layer_22_input_layernorm_weight = "model.layers.22.input_layernorm.weight"
    Layer_22_mlp_down_proj_weight = "model.layers.22.mlp.down_proj.weight"
    Layer_22_mlp_gate_proj_weight = "model.layers.22.mlp.gate_proj.weight"
    Layer_22_mlp_up_proj_weight = "model.layers.22.mlp.up_proj.weight"
    Layer_22_post_attention_layernorm_bias = "model.layers.22.post_attention_layernorm.bias"
    Layer_22_post_attention_layernorm_weight = "model.layers.22.post_attention_layernorm.weight"
    Layer_22_self_attn_k_proj_bias = "model.layers.22.self_attn.k_proj.bias"
    Layer_22_self_attn_k_proj_weight = "model.layers.22.self_attn.k_proj.weight"
    Layer_22_self_attn_o_proj_weight = "model.layers.22.self_attn.o_proj.weight"
    Layer_22_self_attn_q_proj_bias = "model.layers.22.self_attn.q_proj.bias"
    Layer_22_self_attn_q_proj_weight = "model.layers.22.self_attn.q_proj.weight"
    Layer_22_self_attn_v_proj_bias = "model.layers.22.self_attn.v_proj.bias"
    Layer_22_self_attn_v_proj_weight = "model.layers.22.self_attn.v_proj.weight"
    Layer_23_input_layernorm_bias = "model.layers.23.input_layernorm.bias"
    Layer_23_input_layernorm_weight = "model.layers.23.input_layernorm.weight"
    Layer_23_mlp_down_proj_weight = "model.layers.23.mlp.down_proj.weight"
    Layer_23_mlp_gate_proj_weight = "model.layers.23.mlp.gate_proj.weight"
    Layer_23_mlp_up_proj_weight = "model.layers.23.mlp.up_proj.weight"
    Layer_23_post_attention_layernorm_bias = "model.layers.23.post_attention_layernorm.bias"
    Layer_23_post_attention_layernorm_weight = "model.layers.23.post_attention_layernorm.weight"
    Layer_23_self_attn_k_proj_bias = "model.layers.23.self_attn.k_proj.bias"
    Layer_23_self_attn_k_proj_weight = "model.layers.23.self_attn.k_proj.weight"
    Layer_23_self_attn_o_proj_weight = "model.layers.23.self_attn.o_proj.weight"
    Layer_23_self_attn_q_proj_bias = "model.layers.23.self_attn.q_proj.bias"
    Layer_23_self_attn_q_proj_weight = "model.layers.23.self_attn.q_proj.weight"
    Layer_23_self_attn_v_proj_bias = "model.layers.23.self_attn.v_proj.bias"
    Layer_23_self_attn_v_proj_weight = "model.layers.23.self_attn.v_proj.weight"
    Layer_3_input_layernorm_bias = "model.layers.3.input_layernorm.bias"
    Layer_3_input_layernorm_weight = "model.layers.3.input_layernorm.weight"
    Layer_3_mlp_down_proj_weight = "model.layers.3.mlp.down_proj.weight"
    Layer_3_mlp_gate_proj_weight = "model.layers.3.mlp.gate_proj.weight"
    Layer_3_mlp_up_proj_weight = "model.layers.3.mlp.up_proj.weight"
    Layer_3_post_attention_layernorm_bias = "model.layers.3.post_attention_layernorm.bias"
    Layer_3_post_attention_layernorm_weight = "model.layers.3.post_attention_layernorm.weight"
    Layer_3_self_attn_k_proj_bias = "model.layers.3.self_attn.k_proj.bias"
    Layer_3_self_attn_k_proj_weight = "model.layers.3.self_attn.k_proj.weight"
    Layer_3_self_attn_o_proj_weight = "model.layers.3.self_attn.o_proj.weight"
    Layer_3_self_attn_q_proj_bias = "model.layers.3.self_attn.q_proj.bias"
    Layer_3_self_attn_q_proj_weight = "model.layers.3.self_attn.q_proj.weight"
    Layer_3_self_attn_v_proj_bias = "model.layers.3.self_attn.v_proj.bias"
    Layer_3_self_attn_v_proj_weight = "model.layers.3.self_attn.v_proj.weight"
    Layer_4_input_layernorm_bias = "model.layers.4.input_layernorm.bias"
    Layer_4_input_layernorm_weight = "model.layers.4.input_layernorm.weight"
    Layer_4_mlp_down_proj_weight = "model.layers.4.mlp.down_proj.weight"
    Layer_4_mlp_gate_proj_weight = "model.layers.4.mlp.gate_proj.weight"
    Layer_4_mlp_up_proj_weight = "model.layers.4.mlp.up_proj.weight"
    Layer_4_post_attention_layernorm_bias = "model.layers.4.post_attention_layernorm.bias"
    Layer_4_post_attention_layernorm_weight = "model.layers.4.post_attention_layernorm.weight"
    Layer_4_self_attn_k_proj_bias = "model.layers.4.self_attn.k_proj.bias"
    Layer_4_self_attn_k_proj_weight = "model.layers.4.self_attn.k_proj.weight"
    Layer_4_self_attn_o_proj_weight = "model.layers.4.self_attn.o_proj.weight"
    Layer_4_self_attn_q_proj_bias = "model.layers.4.self_attn.q_proj.bias"
    Layer_4_self_attn_q_proj_weight = "model.layers.4.self_attn.q_proj.weight"
    Layer_4_self_attn_v_proj_bias = "model.layers.4.self_attn.v_proj.bias"
    Layer_4_self_attn_v_proj_weight = "model.layers.4.self_attn.v_proj.weight"
    Layer_5_input_layernorm_bias = "model.layers.5.input_layernorm.bias"
    Layer_5_input_layernorm_weight = "model.layers.5.input_layernorm.weight"
    Layer_5_mlp_down_proj_weight = "model.layers.5.mlp.down_proj.weight"
    Layer_5_mlp_gate_proj_weight = "model.layers.5.mlp.gate_proj.weight"
    Layer_5_mlp_up_proj_weight = "model.layers.5.mlp.up_proj.weight"
    Layer_5_post_attention_layernorm_bias = "model.layers.5.post_attention_layernorm.bias"
    Layer_5_post_attention_layernorm_weight = "model.layers.5.post_attention_layernorm.weight"
    Layer_5_self_attn_k_proj_bias = "model.layers.5.self_attn.k_proj.bias"
    Layer_5_self_attn_k_proj_weight = "model.layers.5.self_attn.k_proj.weight"
    Layer_5_self_attn_o_proj_weight = "model.layers.5.self_attn.o_proj.weight"
    Layer_5_self_attn_q_proj_bias = "model.layers.5.self_attn.q_proj.bias"
    Layer_5_self_attn_q_proj_weight = "model.layers.5.self_attn.q_proj.weight"
    Layer_5_self_attn_v_proj_bias = "model.layers.5.self_attn.v_proj.bias"
    Layer_5_self_attn_v_proj_weight = "model.layers.5.self_attn.v_proj.weight"
    Layer_6_input_layernorm_bias = "model.layers.6.input_layernorm.bias"
    Layer_6_input_layernorm_weight = "model.layers.6.input_layernorm.weight"
    Layer_6_mlp_down_proj_weight = "model.layers.6.mlp.down_proj.weight"
    Layer_6_mlp_gate_proj_weight = "model.layers.6.mlp.gate_proj.weight"
    Layer_6_mlp_up_proj_weight = "model.layers.6.mlp.up_proj.weight"
    Layer_6_post_attention_layernorm_bias = "model.layers.6.post_attention_layernorm.bias"
    Layer_6_post_attention_layernorm_weight = "model.layers.6.post_attention_layernorm.weight"
    Layer_6_self_attn_k_proj_bias = "model.layers.6.self_attn.k_proj.bias"
    Layer_6_self_attn_k_proj_weight = "model.layers.6.self_attn.k_proj.weight"
    Layer_6_self_attn_o_proj_weight = "model.layers.6.self_attn.o_proj.weight"
    Layer_6_self_attn_q_proj_bias = "model.layers.6.self_attn.q_proj.bias"
    Layer_6_self_attn_q_proj_weight = "model.layers.6.self_attn.q_proj.weight"
    Layer_6_self_attn_v_proj_bias = "model.layers.6.self_attn.v_proj.bias"
    Layer_6_self_attn_v_proj_weight = "model.layers.6.self_attn.v_proj.weight"
    Layer_7_input_layernorm_bias = "model.layers.7.input_layernorm.bias"
    Layer_7_input_layernorm_weight = "model.layers.7.input_layernorm.weight"
    Layer_7_mlp_down_proj_weight = "model.layers.7.mlp.down_proj.weight"
    Layer_7_mlp_gate_proj_weight = "model.layers.7.mlp.gate_proj.weight"
    Layer_7_mlp_up_proj_weight = "model.layers.7.mlp.up_proj.weight"
    Layer_7_post_attention_layernorm_bias = "model.layers.7.post_attention_layernorm.bias"
    Layer_7_post_attention_layernorm_weight = "model.layers.7.post_attention_layernorm.weight"
    Layer_7_self_attn_k_proj_bias = "model.layers.7.self_attn.k_proj.bias"
    Layer_7_self_attn_k_proj_weight = "model.layers.7.self_attn.k_proj.weight"
    Layer_7_self_attn_o_proj_weight = "model.layers.7.self_attn.o_proj.weight"
    Layer_7_self_attn_q_proj_bias = "model.layers.7.self_attn.q_proj.bias"
    Layer_7_self_attn_q_proj_weight = "model.layers.7.self_attn.q_proj.weight"
    Layer_7_self_attn_v_proj_bias = "model.layers.7.self_attn.v_proj.bias"
    Layer_7_self_attn_v_proj_weight = "model.layers.7.self_attn.v_proj.weight"
    Layer_8_input_layernorm_bias = "model.layers.8.input_layernorm.bias"
    Layer_8_input_layernorm_weight = "model.layers.8.input_layernorm.weight"
    Layer_8_mlp_down_proj_weight = "model.layers.8.mlp.down_proj.weight"
    Layer_8_mlp_gate_proj_weight = "model.layers.8.mlp.gate_proj.weight"
    Layer_8_mlp_up_proj_weight = "model.layers.8.mlp.up_proj.weight"
    Layer_8_post_attention_layernorm_bias = "model.layers.8.post_attention_layernorm.bias"
    Layer_8_post_attention_layernorm_weight = "model.layers.8.post_attention_layernorm.weight"
    Layer_8_self_attn_k_proj_bias = "model.layers.8.self_attn.k_proj.bias"
    Layer_8_self_attn_k_proj_weight = "model.layers.8.self_attn.k_proj.weight"
    Layer_8_self_attn_o_proj_weight = "model.layers.8.self_attn.o_proj.weight"
    Layer_8_self_attn_q_proj_bias = "model.layers.8.self_attn.q_proj.bias"
    Layer_8_self_attn_q_proj_weight = "model.layers.8.self_attn.q_proj.weight"
    Layer_8_self_attn_v_proj_bias = "model.layers.8.self_attn.v_proj.bias"
    Layer_8_self_attn_v_proj_weight = "model.layers.8.self_attn.v_proj.weight"
    Layer_9_input_layernorm_bias = "model.layers.9.input_layernorm.bias"
    Layer_9_input_layernorm_weight = "model.layers.9.input_layernorm.weight"
    Layer_9_mlp_down_proj_weight = "model.layers.9.mlp.down_proj.weight"
    Layer_9_mlp_gate_proj_weight = "model.layers.9.mlp.gate_proj.weight"
    Layer_9_mlp_up_proj_weight = "model.layers.9.mlp.up_proj.weight"
    Layer_9_post_attention_layernorm_bias = "model.layers.9.post_attention_layernorm.bias"
    Layer_9_post_attention_layernorm_weight = "model.layers.9.post_attention_layernorm.weight"
    Layer_9_self_attn_k_proj_bias = "model.layers.9.self_attn.k_proj.bias"
    Layer_9_self_attn_k_proj_weight = "model.layers.9.self_attn.k_proj.weight"
    Layer_9_self_attn_o_proj_weight = "model.layers.9.self_attn.o_proj.weight"
    Layer_9_self_attn_q_proj_bias = "model.layers.9.self_attn.q_proj.bias"
    Layer_9_self_attn_q_proj_weight = "model.layers.9.self_attn.q_proj.weight"
    Layer_9_self_attn_v_proj_bias = "model.layers.9.self_attn.v_proj.bias"
    Layer_9_self_attn_v_proj_weight = "model.layers.9.self_attn.v_proj.weight"
    Norm_bias = "model.norm.bias"
    Norm_weight = "model.norm.weight"


# type BFloat16* = array[2, int8] #not a pointer because it is more lightweight to pass around a value of 2bytes than a pointer of 8bytes

# # converter toBFloat16*(f32: float32): BFloat16 = #this version is for little endianess only
# #     result = cast[BFloat16]((cast[int32](f32) and 0xFFFF0000) shr 16) #just get the first 16 bits

# converter toFloat32*(bf16: BFloat16): float32 = #this version is for little endianess only
#     result = cast[float32]((cast[int32](bf16) and 0x0000FFFF) shl 16) #just zero out the last 16 bits

proc loadWeight(weight: Weights): Matrix =
    let
        weightInfo = weightsInfo[$weight]
        shape = weightInfo["shape"]
        dataOffsets = weightInfo["data_offsets"]
        dataBegin = parseInt $dataOffsets[0]
        dataEnd = parseInt $dataOffsets[1]
        dataSize = dataEnd - dataBegin
        numberOfRows = parseInt $shape[0]

    var 
        numberOfColumns = (if shape.len == 2: parseInt $shape[1] else: 1)
        numberOfBytesReadIn: int
        currentIndex: int

    result = initMat(numberOfRows, numberOfColumns)
    
    if not(isNil(weightsStream)):
        weightsStream.setPosition(dataBegin + OFFSET) #set to beginning of weightData
        while numberOfBytesReadIn < dataSize:
            result.data[currentIndex] = cast[float32](cast[int32](weightsStream.readInt16()) shl 16)
            inc currentIndex 
            inc numberOfBytesReadIn , 2
    else:
        echo "Didnt load the file stream containing the weights"


import times
let then = cputime()

let 
    headWeight = loadWeight(Lm_head_weight) #this takes 18 seconds to load, I want it to happen in 1-2 seconds, will try mmap next
    normBias = loadWeight(Norm_bias)

# echo normBias[0]

echo (headWeight * normBias)[5]

echo "Took ", cputime() - then, " seconds"
